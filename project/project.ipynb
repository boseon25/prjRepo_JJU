{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•´ë‹¹ divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ëŒ€ìƒ URL\n",
    "url = \"https://onstar.jj.ac.kr/\"\n",
    "\n",
    "# ì›¹ í˜ì´ì§€ ìš”ì²­\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# BeautifulSoup ê°ì²´ ìƒì„±\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# íŠ¹ì • div íƒœê·¸ ê°€ì ¸ì˜¤ê¸° (ì˜ˆ: class=\"target-class\"ì¸ div ì°¾ê¸°)\n",
    "div_data = soup.find(\"div\", class_=\"ChildFrame popup\")  # íŠ¹ì • í´ë˜ìŠ¤ê°€ ìˆëŠ” div\n",
    "# ë˜ëŠ” íŠ¹ì • ID\n",
    "# div_data = soup.find(\"div\", id=\"target-id\")\n",
    "\n",
    "# div ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "if div_data:\n",
    "    print(div_data.text.strip())  # í•„ìš”í•˜ë©´ .text ëŒ€ì‹  .get_text(strip=True) ì‚¬ìš©\n",
    "else:\n",
    "    print(\"í•´ë‹¹ divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'í”„ë¡œê·¸ë¨_ì •ë³´': [{'ì£¼ì œ': 'ê¸°ì—…ë¶„ì„ê³¼ ì·¨ì—… ì¤€ë¹„ ì „ëµ', 'ë‚´ìš©': 'ê¸°ì—…ë¶„ì„ê³¼ ì¬ë¬´ë¶„ì„ì˜ ê¸°ì´ˆ, SWOT ë¶„ì„ê³¼ ì „ëµì  ì‚¬ê³  í›ˆë ¨, ì§ë¬´ë¶„ì„ê³¼ ìê¸°ì—­ëŸ‰ ê°œë°œ', 'ê¸°ê°„': '2. 11.(í™”) 14:00~17:00(3ì‹œê°„)', 'ìˆ˜ê°• ì¥ì†Œ': 'ì‹¤ì‹œê°„ ê°•ì˜(zoom)', 'íŠ¹ì´ì‚¬í•­': ['ê°•ì˜ ì§„í–‰ ì „ ë§í¬ ê³µìœ  ì˜ˆì •', 'ì¹´ë©”ë¼ ì–¼êµ´ ë¹„ì¶°ì•¼ ì¶œì„ ì¸ì •, ì´ë™í•˜ë©´ì„œ êµìœ¡ ìˆ˜ê°• ë¶ˆê°€'], 'ë¬¸ì˜ì²˜': '063-220-4654'}, {'êµìœ¡ê¸°ê°„': '2025.02.11 ~ 2025.02.11', 'ì‹ ì²­ê¸°ê°„': '2024.12.30 ~ 2025.02.07', 'ì‹ ì²­ëŒ€ìƒ': 'ì¬í•™ìƒ(íœ´/ë³µí•™ìƒ í¬í•¨), 2ë…„ë‚´ ì¡¸ì—…ì, ë„ë‚´ì²­ë…„ì¸µ', 'êµìœ¡ì¥ì†Œ': 'ê°€ìƒê³µê°„ ì˜¨ë¼ì¸(ì‹¤ì‹œê°„)ê°•ì˜ì‹¤(100)', 'ì°¸ì—¬í˜œíƒ': 'ë¹„êµê³¼ í¬ì¸íŠ¸ ì¸ì •, ì í”„ì—… ìê¸°ì£¼ë„í˜• í¬ì¸íŠ¸ 3ì ', 'ì‹ ì²­êµ¬ë¶„': 'ê°œì¸', 'ë¬¸ì˜ì²˜': '063-220-4654'}]}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27464\\934141581.py:14: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n"
     ]
    }
   ],
   "source": [
    "# JSON íŒŒì¼ ë¡œë“œ\n",
    "json_file = \"programs.json\"\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print(data)\n",
    "\n",
    "# ë¬¸ì„œë¥¼ LangChainì—ì„œ ì‚¬ìš©í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "documents = []\n",
    "for program in data[\"í”„ë¡œê·¸ë¨_ì •ë³´\"]:\n",
    "    text = json.dumps(program, ensure_ascii=False)\n",
    "    documents.append(text)\n",
    "\n",
    "# OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Chroma ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥\n",
    "vectorstore = Chroma.from_texts(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” '2ì›”ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ë¹„êµê³¼ í”„ë¡œê·¸ë¨ ì¶”ì²œí•´ì¤˜' ê´€ë ¨ ì •ë³´:\n",
      "\n",
      " 2024ï¿½ï¿½ï¿½ï¿½ë„ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ 3ï¿½ï¿½(ë©´ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ë‹ˆï¿½ï¿½ï¿½ì´ì…˜ ì „ï¿½ï¿½) ï¿½ï¿½ï¿½ëŠ” JJ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ Day(2ï¿½ï¿½)ë¥¼ ì¶”ï¿½ï¿½í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import OpenAI\n",
    "\n",
    "# GPT-4 ëª¨ë¸ ì—°ê²°\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# RAG ê²€ìƒ‰ ê¸°ë°˜ QA ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "def search_program_info(query):\n",
    "    response = qa_chain.run(query)\n",
    "    return f\"ğŸ” '{query}' ê´€ë ¨ ì •ë³´:\\n\\n{response}\"\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print(search_program_info(\"2ì›”ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ë¹„êµê³¼ í”„ë¡œê·¸ë¨ ì¶”ì²œí•´ì¤˜\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” '2ì›”ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ë¹„êµê³¼ í”„ë¡œê·¸ë¨ ì¶”ì²œí•´ì¤˜' ê´€ë ¨ ì •ë³´:\n",
      "\n",
      " 2024ï¿½ï¿½ï¿½ï¿½ë„ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ 3ï¿½ï¿½(ë©´ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ë‹ˆï¿½ï¿½ï¿½ì´ì…˜ ì „ï¿½ï¿½) í”„ë¡œê·¸ï¿½ï¿½ì´ 2ï¿½ï¿½ 13ì¼ì— ï¿½ï¿½ï¿½ï¿½ë˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤. ì´ í”„ë¡œê·¸ï¿½ï¿½ì€ ë¹„ï¿½ï¿½ê³¼ í¬ì¸íŠ¸ ì¸ì •ê³¼ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ìê¸°ì£¼ë„ï¿½ï¿½ í¬ì¸íŠ¸ 3ï¿½ï¿½ì„ ì œê³µí•˜ï¿½ï¿½, ï¿½ï¿½ï¿½ï¿½ìƒ, 2ï¿½ï¿½ ë‚´ ï¿½ï¿½ï¿½ï¿½ï¿½ì, ï¿½ï¿½ë‚´ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ì„ ëŒ€ìƒìœ¼ë¡œ ï¿½ï¿½ë‹ˆë‹¤. ï¿½ï¿½ì—¬ë¥¼ ï¿½ï¿½í•˜ì‹œë©´ 063-220-4654ë¡œ ë¬¸ì˜í•˜ì‹œë©´ ï¿½ï¿½ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# âœ… .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ë¡œë“œ (UTF-8ë¡œ ê°•ì œ ë³€í™˜í•˜ì—¬ í•œê¸€ ê¹¨ì§ ë°©ì§€)\n",
    "json_file = \"programs.json\"\n",
    "with open(\"programs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"programs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# âœ… ë¬¸ì„œë¥¼ LangChainì—ì„œ ì‚¬ìš©í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (UTF-8 ì¸ì½”ë”© ì ìš©)\n",
    "documents = []\n",
    "for program in data[\"í”„ë¡œê·¸ë¨_ì •ë³´\"]:\n",
    "    text = json.dumps(program, ensure_ascii=False)  # ensure_ascii=False -> í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
    "    documents.append(text)\n",
    "\n",
    "# âœ… OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# âœ… Chroma ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥\n",
    "vectorstore = Chroma.from_texts(documents, embeddings)\n",
    "\n",
    "# âœ… ê²€ìƒ‰ìš© Retriever ìƒì„±\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# âœ… GPT-4 ëª¨ë¸ ì—°ê²°\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# âœ… RAG ê²€ìƒ‰ ê¸°ë°˜ QA ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# âœ… ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ê²°ê³¼ í•œê¸€ ê¹¨ì§ ë°©ì§€)\n",
    "def search_program_info(query):\n",
    "    response = qa_chain.run(query)\n",
    "    \n",
    "    # ğŸ”¥ UTF-8ë¡œ ê°•ì œ ë””ì½”ë”©í•˜ì—¬ í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
    "    try:\n",
    "        response = response.encode('utf-8').decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        response = \"âŒ ê²°ê³¼ë¥¼ ë””ì½”ë”©í•˜ëŠ” ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    return f\"ğŸ” '{query}' ê´€ë ¨ ì •ë³´:\\n\\n{response}\"\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (í•œê¸€ ê¹¨ì§ í™•ì¸)\n",
    "print(search_program_info(\"2ì›”ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ë¹„êµê³¼ í”„ë¡œê·¸ë¨ ì¶”ì²œí•´ì¤˜\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_program_info(query):\n",
    "    response = qa_chain.run(query)\n",
    "    \n",
    "    # í•œê¸€ ê¹¨ì§ ë°©ì§€: ë¶ˆí•„ìš”í•œ ì¸ì½”ë”© ë³€í™˜ ì œê±° & ë””ì½”ë”© ì˜¤ë¥˜ ë°©ì§€\n",
    "    try:\n",
    "        response = response.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        response = \"âŒ ê²°ê³¼ë¥¼ ë””ì½”ë”©í•˜ëŠ” ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    return f\"ğŸ” '{query}' ê´€ë ¨ ì •ë³´:\\n\\n{response}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” '2ì›”ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ë¹„êµê³¼ í”„ë¡œê·¸ë¨ ì¶”ì²œí•´ì¤˜' ê´€ë ¨ ì •ë³´:\n",
      "\n",
      " 2024ï¿½ï¿½ï¿½ï¿½ë„ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ 3ï¿½ï¿½(ë©´ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ë‹ˆï¿½ï¿½ï¿½ì´ì…˜ ì „ï¿½ï¿½) í”„ë¡œê·¸ï¿½ï¿½ì´ 2ï¿½ï¿½ 13ì¼ì— ï¿½ï¿½ï¿½ï¿½ë˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. í•´ë‹¹ í”„ë¡œê·¸ï¿½ï¿½ì€ ë¹„ï¿½ï¿½ê³¼ í¬ì¸íŠ¸ ì¸ì • ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ìê¸°ì£¼ë„ï¿½ï¿½ í¬ì¸íŠ¸ 3ï¿½ï¿½ì„ ì œê³µí•˜ï¿½ï¿½, ï¿½ï¿½ï¿½ï¿½ìƒ, 2ï¿½ï¿½ ë‚´ ï¿½ï¿½ï¿½ï¿½ï¿½ì, ï¿½ï¿½ë‚´ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ì„ ëŒ€ìƒìœ¼ë¡œ ï¿½ï¿½ë‹ˆë‹¤. ë¬¸ì˜ï¿½ï¿½ëŠ” 063-220-4654ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# âœ… .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ë¡œë“œ (UTF-8 ê°•ì œ ë³€í™˜)\n",
    "json_file = \"programs_fixed.json\"  # ì¸ì½”ë”© ë³€í™˜í•œ JSON ì‚¬ìš©\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# âœ… ë¬¸ì„œë¥¼ LangChainì—ì„œ ì‚¬ìš©í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (UTF-8 ì¸ì½”ë”© ì ìš©)\n",
    "documents = []\n",
    "for program in data[\"í”„ë¡œê·¸ë¨_ì •ë³´\"]:\n",
    "    text = json.dumps(program, ensure_ascii=False)  # ensure_ascii=False -> í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
    "    documents.append(text)\n",
    "\n",
    "# âœ… OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# âœ… Chroma ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥\n",
    "vectorstore = Chroma.from_texts(documents, embeddings)\n",
    "\n",
    "# âœ… ê²€ìƒ‰ìš© Retriever ìƒì„±\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# âœ… GPT-4 ëª¨ë¸ ì—°ê²°\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# âœ… RAG ê²€ìƒ‰ ê¸°ë°˜ QA ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# âœ… ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ê²°ê³¼ í•œê¸€ ê¹¨ì§ ë°©ì§€)\n",
    "def search_program_info(query):\n",
    "    response = qa_chain.run(query)\n",
    "    \n",
    "    # í•œê¸€ ê¹¨ì§ ë°©ì§€: ë¶ˆí•„ìš”í•œ ì¸ì½”ë”© ë³€í™˜ ì œê±° & ë””ì½”ë”© ì˜¤ë¥˜ ë°©ì§€\n",
    "    try:\n",
    "        response = response.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        response = \"âŒ ê²°ê³¼ë¥¼ ë””ì½”ë”©í•˜ëŠ” ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    return f\"ğŸ” '{query}' ê´€ë ¨ ì •ë³´:\\n\\n{response}\"\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (í•œê¸€ ê¹¨ì§ í™•ì¸)\n",
    "print(search_program_info(\"2ì›”ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ë¹„êµê³¼ í”„ë¡œê·¸ë¨ ì¶”ì²œí•´ì¤˜\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” '2ì›”ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ë¹„êµê³¼ í”„ë¡œê·¸ë¨ ì¶”ì²œí•´ì¤˜' ê´€ë ¨ ì •ë³´:\n",
      "\n",
      " 2024ï¿½ï¿½ï¿½ï¿½ë„ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ 3ï¿½ï¿½(ë©´ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ë‹ˆï¿½ï¿½ï¿½ì´ì…˜ ì „ï¿½ï¿½) í”„ë¡œê·¸ï¿½ï¿½ì´ 2ï¿½ï¿½ 13ì¼ì— ï¿½ï¿½ï¿½ï¿½ë˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. í•´ë‹¹ í”„ë¡œê·¸ï¿½ï¿½ì€ ë¹„ï¿½ï¿½ê³¼ í¬ì¸íŠ¸ ì¸ì • ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ìê¸°ì£¼ë„ï¿½ï¿½ í¬ì¸íŠ¸ 3ï¿½ï¿½ì„ ì œê³µí•˜ï¿½ï¿½, ï¿½ï¿½ï¿½ï¿½ìƒ, 2ï¿½ï¿½ ë‚´ ï¿½ï¿½ï¿½ï¿½ï¿½ì, ï¿½ï¿½ë‚´ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ì„ ëŒ€ìƒìœ¼ë¡œ ï¿½ï¿½ë‹ˆë‹¤. ë¬¸ì˜ï¿½ï¿½ëŠ” 063-220-4654ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# âœ… .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "\n",
    "# âœ… UTF-8ë¡œ ë³€í™˜í•œ JSON íŒŒì¼ ì‚¬ìš©\n",
    "json_file = \"programs_fixed.json\"\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# âœ… ë¬¸ì„œë¥¼ LangChainì—ì„œ ì‚¬ìš©í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (UTF-8 ì¸ì½”ë”© ì ìš©)\n",
    "documents = []\n",
    "for program in data[\"í”„ë¡œê·¸ë¨_ì •ë³´\"]:\n",
    "    text = json.dumps(program, ensure_ascii=False)  # í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
    "    documents.append(text)\n",
    "\n",
    "# âœ… OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# âœ… Chroma ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥\n",
    "vectorstore = Chroma.from_texts(documents, embeddings)\n",
    "\n",
    "# âœ… ê²€ìƒ‰ìš© Retriever ìƒì„±\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# âœ… GPT-4 ëª¨ë¸ ì—°ê²°\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# âœ… RAG ê²€ìƒ‰ ê¸°ë°˜ QA ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# âœ… ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ê²°ê³¼ í•œê¸€ ê¹¨ì§ ë°©ì§€)\n",
    "def search_program_info(query):\n",
    "    response = qa_chain.run(query)\n",
    "\n",
    "    # ğŸ”¥ í•œê¸€ ê¹¨ì§ ë°©ì§€: JSON ë³€í™˜ ì—†ì´ ê·¸ëŒ€ë¡œ ì¶œë ¥\n",
    "    try:\n",
    "        response = str(response)  # UTF-8 ê°•ì œ ë³€í™˜\n",
    "    except UnicodeDecodeError:\n",
    "        response = \"âŒ ê²°ê³¼ë¥¼ ë””ì½”ë”©í•˜ëŠ” ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    return f\"ğŸ” '{query}' ê´€ë ¨ ì •ë³´:\\n\\n{response}\"\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (í•œê¸€ ê¹¨ì§ í™•ì¸)\n",
    "print(search_program_info(\"2ì›”ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ë¹„êµê³¼ í”„ë¡œê·¸ë¨ ì¶”ì²œí•´ì¤˜\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n",
      "UTF-8\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getdefaultencoding())  # ê¸°ë³¸ ì¸ì½”ë”© í™•ì¸\n",
    "print(sys.stdout.encoding)  # ì¶œë ¥ ì¸ì½”ë”© í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "detach",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# âœ… ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_program_info\u001b[39m(query):\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: detach"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# âœ… .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# âœ… UTF-8ë¡œ ë³€í™˜í•œ JSON íŒŒì¼ ì‚¬ìš©\n",
    "json_file = \"programs_fixed.json\"\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# âœ… ë¬¸ì„œë¥¼ LangChainì—ì„œ ì‚¬ìš©í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (UTF-8 ì¸ì½”ë”© ì ìš©)\n",
    "documents = []\n",
    "for program in data[\"í”„ë¡œê·¸ë¨_ì •ë³´\"]:\n",
    "    text = json.dumps(program, ensure_ascii=False)  # ensure_ascii=False ì ìš©\n",
    "    documents.append(text)\n",
    "\n",
    "# âœ… OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# âœ… Chroma ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥\n",
    "vectorstore = Chroma.from_texts(documents, embeddings)\n",
    "\n",
    "# âœ… ê²€ìƒ‰ìš© Retriever ìƒì„±\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# âœ… GPT-4 ëª¨ë¸ ì—°ê²°\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# âœ… RAG ê²€ìƒ‰ ê¸°ë°˜ QA ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# âœ… í•œê¸€ ê¹¨ì§ ë°©ì§€: UTF-8 ê°•ì œ ì ìš©\n",
    "import sys\n",
    "import io\n",
    "sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')\n",
    "\n",
    "# âœ… ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "def search_program_info(query):\n",
    "    response = qa_chain.run(query)\n",
    "\n",
    "    # ğŸ”¥ í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
    "    try:\n",
    "        response = response.encode('utf-8').decode('utf-8')  \n",
    "    except UnicodeDecodeError:\n",
    "        response = \"âŒ ê²°ê³¼ ë””ì½”ë”© ì˜¤ë¥˜ ë°œìƒ\"\n",
    "\n",
    "    return f\"ğŸ” '{query}' ê´€ë ¨ ì •ë³´:\\n\\n{response}\"\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (í•œê¸€ ê¹¨ì§ í™•ì¸)\n",
    "print(search_program_info(\"2ì›”ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ë¹„êµê³¼ í”„ë¡œê·¸ë¨ ì¶”ì²œí•´ì¤˜\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OutStream' object has no attribute 'buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mreconfigure(encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… ì¶œë ¥ ì¸ì½”ë”© UTF-8ë¡œ ë³€ê²½ ì™„ë£Œ!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# âœ… UTF-8ë¡œ ë³€í™˜í•œ JSON íŒŒì¼ ì‚¬ìš©\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OutStream' object has no attribute 'buffer'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# âœ… .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# âœ… Python ì¶œë ¥ ì¸ì½”ë”© UTF-8ë¡œ ì„¤ì • (detach ë¬¸ì œ í•´ê²°)\n",
    "if hasattr(sys.stdout, \"reconfigure\"):\n",
    "    sys.stdout.reconfigure(encoding=\"utf-8\")\n",
    "else:\n",
    "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding=\"utf-8\")\n",
    "\n",
    "print(\"âœ… ì¶œë ¥ ì¸ì½”ë”© UTF-8ë¡œ ë³€ê²½ ì™„ë£Œ!\")\n",
    "\n",
    "# âœ… UTF-8ë¡œ ë³€í™˜í•œ JSON íŒŒì¼ ì‚¬ìš©\n",
    "json_file = \"programs_fixed.json\"\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# âœ… ë¬¸ì„œë¥¼ LangChainì—ì„œ ì‚¬ìš©í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (UTF-8 ì¸ì½”ë”© ì ìš©)\n",
    "documents = []\n",
    "for program in data[\"í”„ë¡œê·¸ë¨_ì •ë³´\"]:\n",
    "    text = json.dumps(program, ensure_ascii=False)  # í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
    "    documents.append(text)\n",
    "\n",
    "# âœ… OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# âœ… Chroma ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥\n",
    "vectorstore = Chroma.from_texts(documents, embeddings)\n",
    "\n",
    "# âœ… ê²€ìƒ‰ìš© Retriever ìƒì„±\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# âœ… GPT-4 ëª¨ë¸ ì—°ê²°\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# âœ… RAG ê²€ìƒ‰ ê¸°ë°˜ QA ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# âœ… ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "def search_program_info(query):\n",
    "    response = qa_chain.run(query)\n",
    "\n",
    "    # ğŸ”¥ í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
    "    try:\n",
    "        response = response.encode(\"utf-8\").decode(\"utf-8\")  \n",
    "    except UnicodeDecodeError:\n",
    "        response = \"âŒ ê²°ê³¼ ë””ì½”ë”© ì˜¤ë¥˜ ë°œìƒ\"\n",
    "\n",
    "    return f\"ğŸ” '{query}' ê´€ë ¨ ì •ë³´:\\n\\n{response}\"\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (í•œê¸€ ê¹¨ì§ í™•ì¸)\n",
    "print(search_program_info(\"2ì›”ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ë¹„êµê³¼ í”„ë¡œê·¸ë¨ ì¶”ì²œí•´ì¤˜\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# âœ… .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# âœ… Jupyter Notebookì¸ì§€ í™•ì¸ í›„ UTF-8 ì¸ì½”ë”© ì„¤ì •\n",
    "def is_jupyter_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        return get_ipython() is not None\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_jupyter_notebook():\n",
    "    sys.stdout = sys.__stdout__  # Jupyterì—ì„œëŠ” ê¸°ë³¸ ì¶œë ¥ ìœ ì§€\n",
    "    print(\"âœ… Jupyter Notebook í™˜ê²½ì—ì„œ UTF-8 ì„¤ì • ì ìš© ì™„ë£Œ!\")\n",
    "else:\n",
    "    if hasattr(sys.stdout, \"reconfigure\"):\n",
    "        sys.stdout.reconfigure(encoding=\"utf-8\")\n",
    "    else:\n",
    "        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding=\"utf-8\")\n",
    "    print(\"âœ… ì¼ë°˜ Python í™˜ê²½ì—ì„œ UTF-8 ì„¤ì • ì ìš© ì™„ë£Œ!\")\n",
    "\n",
    "# âœ… UTF-8ë¡œ ë³€í™˜í•œ JSON íŒŒì¼ ì‚¬ìš©\n",
    "json_file = \"programs_fixed.json\"\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# âœ… ë¬¸ì„œë¥¼ LangChainì—ì„œ ì‚¬ìš©í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (UTF-8 ì¸ì½”ë”© ì ìš©)\n",
    "documents = []\n",
    "for program in data[\"í”„ë¡œê·¸ë¨_ì •ë³´\"]:\n",
    "    text = json.dumps(program, ensure_ascii=False)  # í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
    "    documents.append(text)\n",
    "\n",
    "# âœ… OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# âœ… Chroma ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥\n",
    "vectorstore = Chroma.from_texts(documents, embeddings)\n",
    "\n",
    "# âœ… ê²€ìƒ‰ìš© Retriever ìƒì„±\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# âœ… GPT-4 ëª¨ë¸ ì—°ê²°\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# âœ… RAG ê²€ìƒ‰ ê¸°ë°˜ QA ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# âœ… ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "def search_program_info(query):\n",
    "    response = qa_chain.run(query)\n",
    "\n",
    "    # ğŸ”¥ í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
    "    try:\n",
    "        response = response.encode(\"utf-8\").decode(\"utf-8\")  \n",
    "    except UnicodeDecodeError:\n",
    "        response = \"âŒ ê²°ê³¼ ë””ì½”ë”© ì˜¤ë¥˜ ë°œìƒ\"\n",
    "\n",
    "    return f\"ğŸ” '{query}' ê´€ë ¨ ì •ë³´:\\n\\n{response}\"\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (í•œê¸€ ê¹¨ì§ í™•ì¸)\n",
    "print(search_program_info(\"2ì›”ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ë¹„êµê³¼ í”„ë¡œê·¸ë¨ ì¶”ì²œí•´ì¤˜\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda (PyTorch)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
